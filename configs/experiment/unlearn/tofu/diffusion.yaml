# @package _global_
# Unlearning with diffusion dLLM (LLaDA or Dream). Use with model override for LLaDA or Dream.
# Example: experiment=unlearn/tofu/diffusion model=LLaDA-8B-Instruct
#          experiment=unlearn/tofu/diffusion model=Dream-Instruct-7B

defaults:
  - override /model: LLaDA-8B-Instruct
  - override /trainer: GradAscent
  - override /data: unlearn
  - override /data/datasets@data.forget: TOFU_QA_forget
  - override /data/datasets@data.retain: TOFU_QA_retain
  - override /eval: tofu

model:
  model_args:
    pretrained_model_name_or_path: "GSAI-ML/LLaDA-8B-Instruct"
  diffusion_adapter:
    training: true
    time_epsilon: 0.001
    loss_weight_type: "scheduler"
    loss_normalization_type: "sequence"

forget_split: forget10
retain_split: retain90
holdout_split: holdout10
retain_logs_path: null
question_key: "question"

eval:
  tofu:
    forget_split: ${forget_split}
    holdout_split: ${holdout_split}
    retain_logs_path: ${retain_logs_path}
    overwrite: true
    question_key: ${question_key}

data:
  anchor: forget
  forget:
    TOFU_QA_forget:
      args:
        hf_args:
          name: ${forget_split}
  retain:
    TOFU_QA_retain:
      args:
        hf_args:
          name: ${retain_split}

trainer:
  args:
    warmup_epochs: 1.0
    learning_rate: 1e-5
    weight_decay: 0.01
    num_train_epochs: 10
    save_strategy: "steps"
    save_steps: 100
    save_total_limit: 3

task_name: ???
